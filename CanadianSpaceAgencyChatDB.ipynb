{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ch73Qp06wwd"
      },
      "source": [
        "### Mock DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQhOOjlK12_l",
        "outputId": "813890ad-cccd-4b02-90b5-41369284d74a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((True, -1), (True, -1))"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import sqlite3\n",
        "import threading\n",
        "import queue\n",
        "\n",
        "def db_worker(db_queue, db_path):\n",
        "    # Connect to the SQLite database in the worker thread\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    c = conn.cursor()\n",
        "    while True:\n",
        "        query, args, result_queue = db_queue.get()\n",
        "        if query is None:\n",
        "            break\n",
        "        try:\n",
        "            c.execute(query, args)\n",
        "            if query.strip().upper().startswith(\"SELECT\"):\n",
        "                result = c.fetchall()\n",
        "            else:\n",
        "                conn.commit()\n",
        "                result = c.rowcount\n",
        "            result_queue.put((True, result))\n",
        "        except Exception as e:\n",
        "            result_queue.put((False, str(e)))\n",
        "    conn.close()\n",
        "\n",
        "def create_tables(db_queue):\n",
        "    result_queue = queue.Queue()\n",
        "    db_queue.put(('''\n",
        "        CREATE TABLE IF NOT EXISTS observation (\n",
        "          observationURI CHAR ,\n",
        "          sequenceNumber INT,\n",
        "          metaReadGroups CHAR,\n",
        "          proposal_keywords CHAR,\n",
        "          target_standard INT,\n",
        "          target_redshift DOUBLE,\n",
        "          target_moving INT,\n",
        "          target_keywords CHAR,\n",
        "          targetPosition_equinox DOUBLE,\n",
        "          targetPosition_coordinates_cval1 DOUBLE,\n",
        "          targetPosition_coordinates_cval2 DOUBLE,\n",
        "          telescope_geoLocationX DOUBLE,\n",
        "          telescope_geoLocationY DOUBLE,\n",
        "          telescope_geoLocationZ DOUBLE,\n",
        "          telescope_keywords CHAR,\n",
        "          instrument_keywords CHAR,\n",
        "          environment_seeing DOUBLE,\n",
        "          environment_humidity DOUBLE,\n",
        "          environment_elevation DOUBLE,\n",
        "          environment_tau DOUBLE,\n",
        "          environment_wavelengthTau DOUBLE,\n",
        "          environment_ambientTemp DOUBLE,\n",
        "          environment_photometric INT,\n",
        "          members CHAR,\n",
        "          typeCode CHAR,\n",
        "          metaProducer CHAR,\n",
        "          metaChecksum CHAR,\n",
        "          accMetaChecksum CHAR,\n",
        "          obsID CHAR PRIMARY KEY,\n",
        "          collection CHAR,\n",
        "          observationID CHAR,\n",
        "          algorithm_name CHAR,\n",
        "          type CHAR,\n",
        "          intent CHAR,\n",
        "          metaRelease CHAR,\n",
        "          proposal_id CHAR,\n",
        "          proposal_pi CHAR,\n",
        "          proposal_project CHAR,\n",
        "          proposal_title CHAR,\n",
        "          target_name CHAR,\n",
        "          target_targetID CHAR,\n",
        "          target_type CHAR,\n",
        "          targetPosition_coordsys CHAR,\n",
        "          telescope_name CHAR,\n",
        "          requirements_flag CHAR,\n",
        "          instrument_name CHAR,\n",
        "          lastModified CHAR,\n",
        "          maxLastModified CHAR\n",
        "      )\n",
        "\n",
        "    ''', (), result_queue))\n",
        "\n",
        "    db_queue.put(('''\n",
        "        CREATE TABLE IF NOT EXISTS plane (\n",
        "          publisherID CHAR,\n",
        "          planeURI CHAR PRIMARY KEY,\n",
        "          creatorID CHAR,\n",
        "          obsID CHAR REFERENCES observation(obsID),\n",
        "          metaReadGroups CHAR,\n",
        "          dataReadGroups CHAR,\n",
        "          calibrationLevel INT,\n",
        "          provenance_keywords CHAR,\n",
        "          provenance_inputs CHAR,\n",
        "          metrics_sourceNumberDensity DOUBLE,\n",
        "          metrics_background DOUBLE,\n",
        "          metrics_backgroundStddev DOUBLE,\n",
        "          metrics_fluxDensityLimit DOUBLE,\n",
        "          metrics_magLimit DOUBLE,\n",
        "          position_bounds CHAR,\n",
        "          position_bounds_samples DOUBLE,\n",
        "          position_bounds_size DOUBLE,\n",
        "          position_resolution DOUBLE,\n",
        "          position_sampleSize DOUBLE,\n",
        "          position_dimension_naxis1 LONG,\n",
        "          position_dimension_naxis2 LONG,\n",
        "          position_timeDependent INT,\n",
        "          energy_bounds_samples DOUBLE,\n",
        "          energy_bounds_lower DOUBLE,\n",
        "          energy_bounds_upper DOUBLE,\n",
        "          energy_bounds_width DOUBLE,\n",
        "          energy_dimension LONG,\n",
        "          energy_resolvingPower DOUBLE,\n",
        "          energy_sampleSize DOUBLE,\n",
        "          energy_freqWidth DOUBLE,\n",
        "          energy_freqSampleSize DOUBLE,\n",
        "          energy_restwav DOUBLE,\n",
        "          time_bounds_samples DOUBLE,\n",
        "          time_bounds_lower DOUBLE,\n",
        "          time_bounds_upper DOUBLE,\n",
        "          time_bounds_width DOUBLE,\n",
        "          time_dimension LONG,\n",
        "          time_resolution DOUBLE,\n",
        "          time_sampleSize DOUBLE,\n",
        "          time_exposure DOUBLE,\n",
        "          polarization_dimension LONG,\n",
        "          custom_bounds_samples DOUBLE,\n",
        "          custom_bounds_lower DOUBLE,\n",
        "          custom_bounds_upper DOUBLE,\n",
        "          custom_bounds_width DOUBLE,\n",
        "          custom_dimension LONG,\n",
        "          metaProducer CHAR,\n",
        "          metaChecksum CHAR,\n",
        "          accMetaChecksum CHAR,\n",
        "          planeID CHAR,\n",
        "          productID CHAR,\n",
        "          metaRelease CHAR,\n",
        "          dataRelease CHAR,\n",
        "          dataProductType CHAR,\n",
        "          provenance_name CHAR,\n",
        "          provenance_version CHAR,\n",
        "          provenance_reference CHAR,\n",
        "          provenance_producer CHAR,\n",
        "          provenance_project CHAR,\n",
        "          provenance_runID CHAR,\n",
        "          provenance_lastExecuted CHAR,\n",
        "          observable_ucd CHAR,\n",
        "          quality_flag CHAR,\n",
        "          position_resolutionBounds DOUBLE,\n",
        "          energy_bounds DOUBLE,\n",
        "          energy_resolvingPowerBounds DOUBLE,\n",
        "          energy_emBand CHAR,\n",
        "          energy_energyBands CHAR,\n",
        "          energy_bandpassName CHAR,\n",
        "          energy_transition_species CHAR,\n",
        "          energy_transition_transition CHAR,\n",
        "          time_bounds DOUBLE,\n",
        "          time_resolutionBounds DOUBLE,\n",
        "          polarization_states CHAR,\n",
        "          custom_ctype CHAR,\n",
        "          custom_bounds DOUBLE,\n",
        "          lastModified CHAR,\n",
        "          maxLastModified CHAR\n",
        "      )\n",
        "    ''', (), result_queue))\n",
        "\n",
        "    return result_queue.get(), result_queue.get()\n",
        "\n",
        "def run_query(db_queue, query, args=()):\n",
        "    result_queue = queue.Queue()\n",
        "    db_queue.put((query, args, result_queue))\n",
        "    success, result = result_queue.get()\n",
        "    if not success:\n",
        "        raise Exception(result)\n",
        "    return result\n",
        "\n",
        "db_queue = queue.Queue()\n",
        "db_path = 'obsplane_db.sqlite'\n",
        "\n",
        "# Start the database worker thread\n",
        "db_thread = threading.Thread(target=db_worker, args=(db_queue, db_path))\n",
        "db_thread.start()\n",
        "\n",
        "# Create tables\n",
        "create_tables(db_queue)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = run_query(db_queue, \"SELECT * FROM observation LIMIT 3;\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK53l8VLMA9Z",
        "outputId": "4d23875e-d3bf-4ec6-9489-cd24e1fef640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fowT5rP-LjX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqlrC8aV-b5s"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGrjTvTC-mul"
      },
      "source": [
        "### Installs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHuaQtMT-rF0"
      },
      "outputs": [],
      "source": [
        " %%capture\n",
        "!pip install -q langchain\n",
        "!pip install -q openai\n",
        "!pip install langchain_community\n",
        "!pip install langchain-experimental\n",
        "!pip install -U langchain-openai\n",
        "!pip install tabulate\n",
        "!pip install langchainhub\n",
        "# Install the requests library if needed\n",
        "!pip install requests\n",
        "!pip install pandas\n",
        "!pip install -qU langchain-openai\n",
        "!pip install -q langchain\n",
        "!pip install langchain_community\n",
        "!pip install langchain-experimental\n",
        "!pip install langchain-core\n",
        "!pip install langchain-text-splitter\n",
        "!pip install langchain-chroma\n",
        "!pip install pypdf\n",
        "!pip install langchainhub\n",
        "!pip install rapidocr-onnxruntime\n",
        "\n",
        "!pip install  openai langchain sentence_transformers chromadb unstructured -q\n",
        "!pip install -U langchain-community\n",
        "!pip install unstructured\n",
        "\n",
        "\n",
        "!pip install -q langchain\n",
        "!pip install -q openai\n",
        "!pip install langchain_community\n",
        "!pip install langchain-experimental\n",
        "!pip install -U langchain-openai\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iUnrYd5-rJ-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y9eXKsA-rPY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWK1OigY-NHJ"
      },
      "source": [
        "### Clone Resources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i0M7AmC-RIq",
        "outputId": "875e9432-972e-4517-ddd6-8ec33ba067c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NRCChatDB'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 25 (delta 2), reused 6 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (25/25), 1.51 MiB | 4.76 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ShaylinThadani/NRCChatDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XAGSAvh-egF"
      },
      "source": [
        "### Run SQL Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHJccG90-jWO"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "@tool\n",
        "def checkSQLMock(sql_code: str) -> str:\n",
        "    \"\"\"Returns a string that states if the SQL query is valid\"\"\"\n",
        "    try:\n",
        "        print(\"Running SQL query...\")\n",
        "        # Attempt to execute the SQL command\n",
        "        run_query(db_queue, sql_code)\n",
        "        return \"Successful! \" + sql_code\n",
        "    except Exception as e:\n",
        "        return f\"SQL query failed with error: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I07dng_q-esa"
      },
      "source": [
        "### Utills for Semantic search and Rag"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMm3BlDxMykQ",
        "outputId": "d010497a-e2bc-4e83-ef55-f3167ca39a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtRFsZFoCwal"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "def load_docs(directory):\n",
        "  loader = DirectoryLoader(directory)\n",
        "  documents = loader.load()\n",
        "  return documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hb30aSDDTTJ"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents.base import Document\n",
        "\n",
        "def split_docs(documents, separators=[\",\"]):\n",
        "    split_documents = []\n",
        "    for document in documents:\n",
        "        parts = document.page_content.split(separators[0])\n",
        "        for part in parts:\n",
        "            split_documents.append(Document(page_content=part.strip(), metadata=document.metadata))\n",
        "    return split_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNnkwBORDbkm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYeFxlVR-e1P"
      },
      "source": [
        "### Valid Columns tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E1y01SBEgZ9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp7anuCjDzwQ"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import os\n",
        "valid_columns_directory = '/content/NRCChatDB/NRCResources/validColumns'\n",
        "\n",
        "valid_columns_documents = load_docs(valid_columns_directory)\n",
        "valid_columns_docs = split_docs(valid_columns_documents)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "valid_columns_vector_db = Chroma.from_documents(valid_columns_docs, OpenAIEmbeddings())\n",
        "@tool\n",
        "def checkColumns(columns: str) -> str:\n",
        "    \"\"\"Accepts a string of columns separated by comas and checks which table the columns belong to. It returns a string representation of a dictionary where the key is the column and the value is either {column does not exist, In Observation table, In Plane table} \"\"\"\n",
        "    print(\"checked columns\")\n",
        "    columns = columns.split(\",\")\n",
        "    columns = [column.strip() for column in columns]\n",
        "    obs_columns_set = {'telescope_name', 'target_type', 'type', 'metaReadGroups', 'target_moving', 'collection', 'environment_ambientTemp', 'target_name', 'target_keywords', 'metaChecksum', 'environment_wavelengthTau', 'telescope_geoLocationZ', 'instrument_keywords', 'obsID', 'proposal_title', 'environment_tau', 'targetPosition_coordinates_cval2', 'typeCode', 'metaProducer', 'observationID', 'proposal_project', 'telescope_geoLocationX', 'proposal_pi', 'targetPosition_coordsys', 'accMetaChecksum', 'instrument_name', 'lastModified', 'requirements_flag', 'target_targetID', 'sequenceNumber', 'targetPosition_coordinates_cval1', 'environment_humidity', 'intent', 'algorithm_name', 'maxLastModified', 'telescope_keywords', 'environment_elevation', 'target_standard', 'telescope_geoLocationY', 'members', 'targetPosition_equinox', 'environment_seeing', 'metaRelease', 'proposal_id', 'target_redshift', 'environment_photometric', 'observationURI', 'proposal_keywords'}\n",
        "    plane_columns_set= {'metaReadGroups', 'time_bounds_samples', 'energy_freqWidth', 'position_bounds_samples', 'dataProductType', 'position_dimension_naxis2', 'provenance_runID', 'lastModified', 'energy_resolvingPower', 'provenance_name', 'position_sampleSize', 'energy_bounds_width', 'position_dimension_naxis1', 'energy_sampleSize', 'metrics_magLimit', 'time_resolutionBounds', 'provenance_producer', 'energy_resolvingPowerBounds', 'energy_bounds_upper', 'provenance_inputs', 'provenance_keywords', 'time_bounds_upper', 'polarization_states', 'energy_restwav', 'position_timeDependent', 'energy_dimension', 'metaProducer', 'quality_flag', 'custom_bounds_upper', 'energy_transition_transition', 'energy_bounds', 'time_bounds_width', 'provenance_version', 'planeID', 'custom_bounds_samples', 'planeURI', 'creatorID', 'metaRelease', 'observable_ucd', 'custom_bounds', 'provenance_reference', 'energy_bounds_lower', 'publisherID', 'metrics_background', 'polarization_dimension', 'custom_dimension', 'metaChecksum', 'time_resolution', 'provenance_project', 'obsID', 'metrics_backgroundStddev', 'dataRelease', 'accMetaChecksum', 'time_sampleSize', 'position_resolutionBounds', 'time_bounds', 'time_dimension', 'calibrationLevel', 'energy_transition_species', 'custom_ctype', 'metrics_sourceNumberDensity', 'energy_bounds_samples', 'energy_emBand', 'position_resolution', 'metrics_fluxDensityLimit', 'position_bounds_size', 'dataReadGroups', 'custom_bounds_width', 'custom_bounds_lower', 'productID', 'energy_freqSampleSize', 'time_exposure', 'energy_bandpassName', 'position_bounds', 'maxLastModified', 'provenance_lastExecuted', 'time_bounds_lower', 'energy_energyBands'}\n",
        "\n",
        "    result = dict()\n",
        "    for column in columns:\n",
        "        if column not in obs_columns_set and column not in plane_columns_set:\n",
        "            query = \"What is the most similar to \" + column + \"?\"\n",
        "            matching_docs = valid_columns_vector_db.similarity_search(query)\n",
        "            page_contents = [doc.page_content for doc in matching_docs[:10]]\n",
        "            result[column] = \"column does not exist, try one of these\" + str(page_contents)\n",
        "            continue\n",
        "        if column in obs_columns_set:\n",
        "            result[column] = \"In Observation table\"\n",
        "        else:\n",
        "            result[column] = \"In Plane table\"\n",
        "    print(str(result))\n",
        "    return str(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayWHWQpm-e-X"
      },
      "source": [
        "### Column Mapping Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL_w8w4eEmf7"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "map_columns_directory = '/content/NRCChatDB/NRCResources/columnmappings'\n",
        "\n",
        "map_columns_documents = load_docs(map_columns_directory)\n",
        "map_columns_docs = split_docs(map_columns_documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUjJrLX1Dq4c"
      },
      "outputs": [],
      "source": [
        "import bs4\n",
        "import requests\n",
        "import os\n",
        "from langchain import hub\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# Load, chunk and index the contents of the blog.\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "map_columns_vector_db = Chroma.from_documents(documents=map_columns_docs, embedding=OpenAIEmbeddings())\n",
        "\n",
        "# Retrieve and generate using the relevant snippets of the blog.\n",
        "retriever = map_columns_vector_db.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "map_columns_llm= OpenAI(api_key=\"\")\n",
        "\n",
        "\n",
        "map_columns_rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | map_columns_llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aMANFA6F5pC"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def alternateColumn(column: str) -> str:\n",
        "    \"\"\"Checks to see if a column needs to be changes to an alternate column name\"\"\"\n",
        "    result = map_columns_rag_chain.invoke(\"Check to see if this word is before 'maps to', and if it is what is it mapped to: \" + column)\n",
        "    print(\"used alternate column\")\n",
        "    print(result)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTU_kobYF-Kj"
      },
      "source": [
        "### Value Synonyms Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKD_nJ29GEra"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "value_synonyms_directory = '/content/NRCChatDB/NRCResources/valueSynonymMappings'\n",
        "\n",
        "value_synonyms_documents = load_docs(value_synonyms_directory)\n",
        "value_synonyms_docs = split_docs(value_synonyms_documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX9kYW0HGVPf"
      },
      "outputs": [],
      "source": [
        "import bs4\n",
        "import requests\n",
        "import os\n",
        "from langchain import hub\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# Load, chunk and index the contents of the blog.\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "value_synonyms_vector_db = Chroma.from_documents(documents=value_synonyms_docs, embedding=OpenAIEmbeddings())\n",
        "\n",
        "# Retrieve and generate using the relevant snippets of the blog.\n",
        "retriever = value_synonyms_vector_db.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "value_synonyms_llm= OpenAI(api_key=\"\")\n",
        "\n",
        "\n",
        "value_synonyms_rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | value_synonyms_llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b95tSG7YGuge"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "@tool\n",
        "def alternateValue(value: str) -> str:\n",
        "    \"\"\"Checks to see if a value needs to be changed to an alternate value name\"\"\"\n",
        "    result = value_synonyms_rag_chain.invoke(\"Check to see if this word maps to another: \" + value)\n",
        "    print(\"used alternate value tool\")\n",
        "    print(result)\n",
        "    print(\"value passed in\")\n",
        "    print(value)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Schema"
      ],
      "metadata": {
        "id": "qJhnCgysNZlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from langchain.tools import tool\n",
        "pd.set_option('display.max_rows', None)\n",
        "def get_table_schema(table_name: str) -> pd.DataFrame:\n",
        "    \"\"\"Parses the VOTABLE XML response to extract table schema information.\"\"\"\n",
        "    headers = {'Accept': 'application/x-votable+xml'}\n",
        "    response = requests.get(f\"https://ws.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/argus/sync?LANG=ADQL&QUERY=SELECT%20*%20FROM%20{table_name}%20LIMIT%200\", headers=headers)\n",
        "    xml_response = response.content.decode('utf-8')  # Decode the response content\n",
        "\n",
        "    root = ET.fromstring(xml_response)\n",
        "\n",
        "    columns = []\n",
        "    types = []\n",
        "    descriptions = []\n",
        "\n",
        "    for field in root.findall(\".//{http://www.ivoa.net/xml/VOTable/v1.3}FIELD\"):\n",
        "        columns.append(field.attrib['name'])\n",
        "        types.append(field.attrib['datatype'])\n",
        "        descriptions.append(field.find(\"{http://www.ivoa.net/xml/VOTable/v1.3}DESCRIPTION\").text)\n",
        "\n",
        "    schema_df = pd.DataFrame({\n",
        "        'Column': columns,\n",
        "        'Type': types,\n",
        "        'Description': descriptions\n",
        "    })\n",
        "\n",
        "    return schema_df\n",
        "\n",
        "def format_schemas(*schema_dfs) -> str:\n",
        "    schema_str = \"Table Schema:\\n\"\n",
        "    for schema_df in schema_dfs:\n",
        "        table_name = schema_df['Table'].iloc[0]\n",
        "        schema_str += f\"Table: {table_name}\\n\"\n",
        "        for index, row in schema_df.iterrows():\n",
        "            schema_str += f\"  Column: {row['Column']} - Type: {row['Type']} - Description: {row['Description']}\\n\"\n",
        "    schema_str += \"Join Condition: caom2.Plane.obsID = caom2.Observation.obsID\\n\"\n",
        "    return schema_str\n",
        "\n",
        "# get the two table schemas\n",
        "obs_schema = get_table_schema(\"caom2.Observation\")\n",
        "plane_schema = get_table_schema(\"caom2.Plane\")\n",
        "# name the tables\n",
        "obs_schema['Table'] = 'observation' # Changed table names\n",
        "plane_schema['Table'] = 'plane'\n",
        "# print tables\n",
        "display(obs_schema)\n",
        "display(plane_schema)\n",
        "# format schemas into a string to pass to sql generator\n",
        "combined_schema_str = format_schemas(obs_schema, plane_schema)"
      ],
      "metadata": {
        "id": "UyZLMkPnNb2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_react_prompt_template():\n",
        "    # Get the react prompt template\n",
        "    return PromptTemplate.from_template(\"\"\"Answer the following questions as accurately as possible. Your primary task is to enhance the user's input by ensuring it is complete, accurate, and consistent with the database schema. You will need to augment and improve the input to add the correct table names to the prompt, change or swap out any incorrect column names and change any values that have an alternate value. To achieve this, follow these instructions carefully:\n",
        "\n",
        "1. **alternateColumn**: Always start by verifying if any column names in the user's input need to be updated or corrected based on the database schema. Use this tool to find the correct column names if needed.\n",
        "2. **checkColumns**: Next, ensure that all mentioned columns exist in the specified table(s). Confirm which columns belong to which table(s) and that the columns in the user's input are valid.\n",
        "3. **alternateValue**: Finally, check if any values (like specific names or IDs) in the user's input need to be corrected. Use this tool to verify and update values as necessary.\n",
        "\n",
        "**Important:** You must use these tools in the order provided for every query.\n",
        "\n",
        "\n",
        "    You have access to the following tools:\n",
        "\n",
        "    {tools}\n",
        "\n",
        "    Use the following format:\n",
        "\n",
        "    Question: the input question you must answer\n",
        "    Thought: you should always think about what to do\n",
        "    Action: the action to take, should be one of [{tool_names}]\n",
        "    Action Input: the input to the action\n",
        "    Observation: the result of the action\n",
        "    ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "    Thought: I now know the final answer\n",
        "    Final Answer: the final answer to the original input question\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    Question: {input}\n",
        "    Thought:{agent_scratchpad}\n",
        "    \"\"\")"
      ],
      "metadata": {
        "id": "COeeHCXFQqC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Agent"
      ],
      "metadata": {
        "id": "8dshaGjLMnST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent # try react\n",
        "from langchain_openai import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "from tabulate import tabulate\n",
        "from IPython.display import display\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain import hub\n",
        "\n",
        "# Define a prompt template to generate SQL from natural language questions\n",
        "template = \"\"\"\n",
        "You are an expert SQL query generator. Given a natural language question and a table schema, generate an appropriate SQL query.\n",
        "Only return the SQL query and nothing else.\n",
        "{schema}\n",
        "Question: {question}\n",
        "SQL Query:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"schema\", \"question\"], template=template)\n",
        "\n",
        "# Initialize the OpenAI LLM (replace with your actual API key)\n",
        "llm = OpenAI()\n",
        "\n",
        "# Define a function that uses the LLM to generate SQL\n",
        "def generate_sql(question: str, schema: str) -> str:\n",
        "    response = llm.invoke(prompt.format(schema=schema, question=question))\n",
        "    sql_query = response.strip()\n",
        "    return sql_query\n",
        "\n",
        "class SQLAgent:\n",
        "    def __init__(self, sql_tool, sql_generator):\n",
        "        self.sql_tool = sql_tool\n",
        "        self.sql_generator = sql_generator\n",
        "        self.max_retries = 3\n",
        "        self.preprocessAgent = self.createPreprocessAgent()\n",
        "\n",
        "    def createPreprocessAgent(self):\n",
        "        # Choose the LLM to use\n",
        "        llm2 = OpenAI(api_key=\"\")\n",
        "\n",
        "        # Get the react prompt template\n",
        "        prompt_template = get_react_prompt_template()\n",
        "\n",
        "\n",
        "        # set the tools\n",
        "        tools = [alternateColumn,checkColumns, alternateValue]\n",
        "\n",
        "        # Construct the ReAct agent\n",
        "        agent = create_react_agent(llm2, tools, prompt_template)\n",
        "\n",
        "        # Create an agent executor by passing in the agent and tools\n",
        "        return AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
        "    def preprocess(self, input: str) -> str:\n",
        "        improved_input = self.preprocessAgent.invoke({\"input\": input})\n",
        "        return improved_input\n",
        "\n",
        "    def regenerate_sql(self, question: str, schema: str, error_messages: list, previous_queries: list) -> str:\n",
        "        # Create a new prompt that includes the error messages and previous queries to guide the LLM in fixing the query\n",
        "        regenerate_prompt = \"\"\"\n",
        "        You are an expert SQL query generator. Given a natural language question, a table schema, previous error messages, and previous SQL queries, generate a corrected SQL query.\n",
        "        Only return the SQL query and nothing else. Do not include a semicolon at the end of the query.\n",
        "\n",
        "        Schema:\n",
        "        {schema}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Error messages:\n",
        "        {error_messages}\n",
        "\n",
        "        Previous SQL queries:\n",
        "        {previous_queries}\n",
        "\n",
        "        Corrected SQL Query:\n",
        "        \"\"\"\n",
        "        error_messages_str = '\\n'.join(error_messages)\n",
        "        previous_queries_str = '\\n'.join(previous_queries)\n",
        "        prompt = PromptTemplate(input_variables=[\"schema\", \"question\", \"error_messages\", \"previous_queries\"], template=regenerate_prompt)\n",
        "        response = llm.invoke(prompt.format(schema=schema, question=question, error_messages=error_messages_str, previous_queries=previous_queries_str))\n",
        "        sql_query = response.strip().strip(\";\")\n",
        "        if sql_query[-1] == \";\":\n",
        "          sql_query = sql_query[:-1]\n",
        "        return sql_query\n",
        "\n",
        "    def run(self, question: str) -> pd.DataFrame:\n",
        "        schema = combined_schema_str\n",
        "        error_messages = []\n",
        "        previous_queries = []\n",
        "        print(\"orignal question\")\n",
        "        print(question)\n",
        "        question = self.preprocess(question)[\"output\"]\n",
        "        print(\"augmented question\")\n",
        "        print(question)\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                # Generate the SQL query from the question and schema\n",
        "                sql_query = self.sql_generator(question, schema)\n",
        "                print(f\"Attempt {attempt+1}: Generated query: {sql_query}\")\n",
        "                previous_queries.append(sql_query)\n",
        "                # Run the generated SQL query using the tool\n",
        "                return self.sql_tool.invoke(sql_query)\n",
        "            except Exception as e:\n",
        "                error_message = str(e)\n",
        "                print(f\"Attempt {attempt+1}: Error encountered: {error_message}\")\n",
        "                error_messages.append(error_message)\n",
        "                # Regenerate the SQL query with the error message and previous queries\n",
        "                sql_query = self.regenerate_sql(question, schema, error_messages, previous_queries)\n",
        "        raise Exception(\"Failed to generate a correct SQL query after \" + str(self.max_retries) + \" attempts.\")\n",
        "# Initialize the agent\n",
        "agent = SQLAgent(checkSQLMock, generate_sql)\n"
      ],
      "metadata": {
        "id": "fV62YUFWMrww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d__4wCCLd70"
      },
      "source": [
        "### Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttwDk89RLhRe"
      },
      "outputs": [],
      "source": [
        "agent.run(\"I need the first 5 entries in the observation table where the collection column value is Hubble\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# english question\n",
        "# did not use the check columns tool and made up a table\n",
        "agent.run(\"Show the names of instruments and filters for observations with a calibration level of '2' and type 'ALIGN'\")"
      ],
      "metadata": {
        "id": "-0kpWMVsVLGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# synonyms\n",
        "agent.run(\"Find the first 20 rows of the instrument_name column where the instrument_name is B-Three, Band IV, Band V, B-Six, or Band VII in the observation table.\")"
      ],
      "metadata": {
        "id": "AJ3w_qLiVLJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter column needs to be mapped\n",
        "agent.run(\"I need the first 5 entries in the observation table where the filter column value is C2\")"
      ],
      "metadata": {
        "id": "kZkXpowtVLL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter column needs to be mapped\n",
        "agent.run(\"I need the first 5 entries in the observation table where the filter column value is C2 and instrument_name is B-Three, Band IV, Band V, B-Six, or Band VII\")"
      ],
      "metadata": {
        "id": "K_EhcBBlWTx0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}